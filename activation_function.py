# -*- coding: utf-8 -*-
"""activation-function.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wqDB0pbniK1ijXORLR1t6E8esQNsYXJv
"""

# Commented out IPython magic to ensure Python compatibility.
import math
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

# In a neural network neurons are what make up the layers
# In the input layer each neuron represents the features from the input
# These are then passed on to the next layer
# In the hidden & output layers
# Each neuron gets an input and multiplicatively applies the weights (that are connected to it) summing all of the results
# Then it adds a bias term to this result, shifting it in a direction (giving more flexibility)
# This result is taken to the activation function
# There are lots of common activation functions like sigmoid, reLu, and tanh
# Overall they usually squash the results giving an output of a range dependent on function used (ex. -1 to 1)
# finally what comes out is the output from the activation function
# This is passed on to the next neuron until they reach the output layer and softmax is applied

plt.plot(np.arange(-5, 5, 0.2), np.tanh(np.arange(-5, 5, 0.2))); plt.grid();
# Inputs get squashed on the y-coord to be in a range of -1 to 1, this is the tanh activation function

class Value:
  """
  Takes a single scalar value, to keep track of (essentially wraps it)

  Scalar value is taking in via data, and it is wrapped in the class via -> self.data

  The double underscore is used to define the operators of the functions, and are automatically called via interepreter
  aka magic function. This allows them to replace certain things or have things happen in a manner when something is done.

  Example:
  __repr__: provides the string representation of the object
  __add__: allows u to define the behavior of the addition operator when used with objects of the class
  """
  def __init__(self, data, _children=(), _op=''): # creating children as an immutable list
    self.data = data
    self.grad = 0.0 # Maintains the derivative of the output/loss func in respect to the current val
    # Being intialized to zero, so we assume it doesnt have an effect on the output

    self._prev = set(_children) # set to remove duplicates
  #  Need connective tissue, and want to make this a graph --
  # so we need pointers to know what values create other values
  # Thus prev is introduced
    self._op = _op
    self._backward = lambda: None # Intially a function that doesnt do anything, like for a leaf node theres nothing to do


  def __repr__(self):
    return f"Value(data={self.data})"

  def __add__(self, other):
    """
    Takes another value, and takes its sellf and then gets its own value and adds it to the other and returns it
    """
    out = Value(self.data + other.data, (self, other), '+') # feed in children of the val, which is a tuple of itself and other
    def _backward():
      self.grad = 1.0 * out.grad
      other.grad = 1.0 * out.grad
    out._backward = _backward
    return out

  def __mul__(self, other):
    out = Value(self.data * other.data , (self, other), '*') # feed in children of the val, which is a tuple of itself and other
    def _backward():
      self.grad = other.data * out.grad
      other.grad = self.data * out.grad
    out._backward = _backward
    return out

  def tanh(self):
    x = self.data
    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)
    out = Value(t, (self, ), 'tanh')

    def _backward():
      self.grad = (1 - t**2) * out.grad
    out._backward = _backward
    return out

# 2d neuron,

# with the inputs x1 and x2

x1 = Value(2.0)
x2 = Value(0.0)

# weights w1,w2 of the neuron--the ones connecting them essentially

w1 = Value(-3.0)
w2 = Value(1.0)

# bias of the neuron to shift neuron and create flexibility
b = Value(6.8813735870195432)

# input 1 times weight 1, and input 2 times weight 2; sum those; add the biases on top
x1w1 = x1*w1
x2w2 = x2*w2
x1w1x2w2 = x1w1 + x2w2
n = x1w1x2w2 + b

# So up to here we basically took the inputs & multiplied them by the weights, summed it, and added the biases on top
# Now we need to run this through the activation function

o = n.tanh()
print(o)
# Squashed the input

o.grad = 1.0

# o = tanh(n)