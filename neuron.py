# -*- coding: utf-8 -*-
"""neuron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dVLMqvJ3bm8LlX1s_kkef7JG5CSp1-WL
"""

# Commented out IPython magic to ensure Python compatibility.
import math
import random
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
# %matplotlib inline

# In PyTorch everything is based around Tensors
# Tensors are just n-dimensional arrays of scalars, i.e arrays of numbers with multiple dimensions, meaning they can have array in array etc

x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True
x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True
w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True
w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True
b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True
n = x1*w1 + x2*w2 + b
o = torch.tanh(n)

print(o.data.item())
o.backward()

print('---')
print('x2', x2.grad.item())
print('w2', w2.grad.item())
print('x1', x1.grad.item())
print('w1', w1.grad.item())

torch.Tensor([[1,2,3], [4,5,6]]).shape
# 2 by 3 array

class Neuron(nn.Module):
  # This Neuron class will have all the features of nn.Module (aka inherits). This is the base class for all nn in pytorch
  def __init__(self, nin):
    """
    nin = number of inputs come to a neuron
    """
    super(Neuron, self).__init__()
    # Shape is nin to indiciate one weight per input
    # These are learnable

    self.w = nn.Parameter(torch.Tensor(nin).uniform_(-1, 1)) # For every input we create a random weight between -1 and 1
    # Random bias value
    self.b = nn.Parameter(torch.Tensor(1).uniform_(-1, 1)) # Bias that controls trigger happiness (also random)

  def __call__(self, x):
    # Replaces the () when calling after instantiation
    # zipped = zip(self.w,x). # Zip the weights and inputs. What zip does is take two iterators, and creates a new iterator that can iterate
                   # over the tuples of the corresponding entries
                   # Ex. will be the w and x paired up

    # Now we can loop through it and multiply the two together, then sum them
    # activation = sum(wi*xi for wi,xi in zipped, self.b)
    # out = activation.tanh()
    activation = torch.sum(self.w * x) + self.b # Weighted sum plus bias
    out = torch.tanh(activation) # Activation function (Tanh)
    return out

nin = 3  # Number of inputs
neuron = Neuron(nin)

# Create a sample input
x = torch.Tensor([0.5, -0.2, 0.1])

# Get the output of the neuron
output = neuron(x)
print(output.item())

